{
    // Model config
    "experiment_type": "seq2seq",
    "model_name_or_path": "google/pegasus-cnn_dailymail",
    "max_source_length": 1024,
    "max_target_length": 512,
    "output_dir": "models/pegasus_cnn_dm/1024__with_highlights__no_sent_filter",
    "should_preprocess_add_highlights": true,
    "should_preprocess_only_sents_with_highlights": false,
    "overwrite_cache": true,  // add this if getting the error CUBLAS_STATUS_ALLOC_FAILED (happened because of change in positional embeddings size)
    "eval_with_summac": false,
    // Model configs copied from https://github.com/huggingface/transformers/blob/main/examples/research_projects/seq2seq-distillation/finetune_pegasus_xsum.sh
    "learning_rate": 0.0001,
    "freeze_embeds": true, // For faster training / larger batch-size
    "fp16": true, // Lower memory consumption and faster training (not supported based on docs)
    "gradient_checkpointing": true, // Slower but allows more memory to be allocated
    "label_smoothing": 0.1,
    "adafactor": true, // recommended by official page
    // Predict
    "predict_with_generate": "true",
    // Train
    "do_train": true,
    "per_device_train_batch_size": 2,
    "gradient_accumulation_steps": 2, // will accumulate gradient_accumulation_steps * train_batch_size * num_gpus
    "overwrite_output_dir": "true",
    "train_file": "data/train__highlights.csv",
    "save_total_limit": 2,  // Save only last one and best one
    "metric_for_best_model": "eval_gold_rouge1",
    "load_best_model_at_end": true,
    // Eval while training
    "do_eval": true,
    "per_device_eval_batch_size": 1,  // Sometimes can be larger than training batch size (no grad is activated)
    "validation_file": "data/dev__highlights.csv",
    "evaluation_strategy": "steps",
    "save_steps": 100,
    "eval_steps": 100,
    "logging_steps": 50,
    "num_train_epochs": 10.0,
    "num_beams": 4,
    // Wandb
    "report_to": "wandb",
    "run_name": "pegasus_cnn_dm/1024__with_highlights__no_sent_filter"
}