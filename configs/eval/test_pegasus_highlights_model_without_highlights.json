{
    // Model config
    "experiment_type": "seq2seq",      
    // "model_name_or_path": "./models/pegasus_cnn_dm/1024__with_highlights/checkpoint-1200",
    "model_name_or_path": "./models/pegasus_cnn_dm/1024__with_highlights__no_sent_filter/checkpoint-1000",
    // "model_name_or_path": "./models/pegasus_cnn_dm/1024__no_highlights/checkpoint-300",
    "output_dir": "models/predict/pegasus/test_without_highlights",
    "max_source_length": 1024,
    "max_target_length": 512,  // Lowering this will yield unreliable rouge results (based only on the limited summary)!
    "fp16": true, // Lower memory consumption and faster training (not supported based on docs)
    "overwrite_cache": true,  // add this if getting the error CUBLAS_STATUS_ALLOC_FAILED
    "eval_with_summac": false,
    // Pegasus
    // "should_preprocess_add_highlights": true,
    // "should_preprocess_only_sents_with_highlights": true,
    // Train config                
    "overwrite_output_dir": "true",
    "validation_file": "data/test__highlights.csv", // Necessary just not to crash run.py, doesn't do anything
    // Predict
    "do_predict": true,
    "predict_with_generate": "true",
    "test_file": "data/test__highlights.csv",
    "per_device_eval_batch_size": 1,  // Sometimes can be larger than training batch size (no grad is activated)
    "num_beams": 4,  // Lever to play with if getting OOM
    // Cancel Wandb
    "report_to": "none"
}