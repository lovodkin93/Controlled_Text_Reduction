{// Model config
"experiment_type": "seq2seq",
"model_name_or_path": "google/pegasus-cnn_dailymail",
"max_source_length": 512,
"max_target_length": 512,
"output_dir": "${env:TMPDIR}/controlled_reduction/google/pegasus-cnn_dailymail/model",
// Model configs copied from https://github.com/huggingface/transformers/blob/main/examples/research_projects/seq2seq-distillation/finetune_pegasus_xsum.sh
"learning_rate": "1e-4",
"freeze_embeds": true, // For faster training / larger batch-size
"fp16": true, // Lower memory consumption and faster training (not supported based on docs)
// "gradient_checkpointing", // Slower but allows more memory to be allocated
"label_smoothing": 0.1,
"adafactor": true, // recommended by official page
// Predict
// "do_predict",  # Loading predict dataset just loads the memory un-necessairly
"predict_with_generate": "true",  
// "test_file": "data/test.csv",
// Train              
"do_train": true,
"per_device_train_batch_size": 1,
"overwrite_output_dir": "true",
"train_file": "data/train__highlights.csv",
"save_total_limit": 2,  // Save only laste one and best one
// Eval while training
"do_eval": true,
"per_device_eval_batch_size": 1,  // Sometimes can be larger than training batch size (no grad is activated)
"validation_file": "data/dev__highlights.csv",
"evaluation_strategy": "steps",
"eval_steps": 100,
"logging_steps": 50,
"num_train_epochs": 10.0,
// Wandb
"report_to": "wandb",
"run_name": "google/pegasus-cnn_dailymail"
}