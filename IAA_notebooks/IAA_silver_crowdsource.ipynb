{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8bafc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "import os\n",
    "import string \n",
    "from itertools import combinations\n",
    "import spacy\n",
    "import pickle\n",
    "import string \n",
    "import logging\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75961f7b",
   "metadata": {},
   "source": [
    "## paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c502c4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "indir_mturk = r\"C:\\Users\\aviv\\OneDrive\\Desktop\\controlled_reduction_production\\cleaned_data\\second_batch\\mturk_alignments.csv\"\n",
    "indir_superpal = r\"C:\\Users\\aviv\\OneDrive\\Desktop\\controlled_reduction_production\\cleaned_data\\second_batch\\for_annotator_parser_agreement\\filtered_checkpoint_2000.csv\"\n",
    "indir_spacy_tokenization = r\"C:\\Users\\aviv\\OneDrive\\Desktop\\controlled_reduction_production\\cleaned_data\\second_batch\\spacy_tokenization.json\"\n",
    "outdir = r\"C:\\Users\\aviv\\OneDrive\\Desktop\\controlled_reduction_production\\cleaned_data\\second_batch\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5881d9",
   "metadata": {},
   "source": [
    "## reading content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d459ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "mturk_df = pd.read_csv(indir_mturk)\n",
    "\n",
    "superpal_df = pd.read_csv(indir_superpal)\n",
    "\n",
    "with open(indir_spacy_tokenization) as f1:\n",
    "    spacy_jsons = json.loads(f1.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4925b113",
   "metadata": {},
   "source": [
    "## Calculate IAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a07b6ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def span_to_str(subspans):\n",
    "    output_str = \"\"\n",
    "    for subspan in subspans:\n",
    "        output_str = output_str + f\"{str(subspan[0])}, {str(subspan[1])};\"\n",
    "    return output_str[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8bbf3fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_consecutive_subspans(idx_list):\n",
    "    if not idx_list:\n",
    "        return []\n",
    "    idx_subspans = []\n",
    "    low_lim, up_lim = -1, -1\n",
    "    for i in range(len(idx_list)-1):\n",
    "        if low_lim == -1:\n",
    "            low_lim = idx_list[i]\n",
    "            up_lim = -1\n",
    "        if idx_list[i+1] > idx_list[i]+1:\n",
    "            up_lim = idx_list[i]\n",
    "            idx_subspans.append([low_lim, up_lim])\n",
    "            low_lim = -1\n",
    "    if low_lim == -1:\n",
    "        idx_subspans.append([idx_list[-1], idx_list[-1]])\n",
    "    else:\n",
    "        idx_subspans.append([low_lim, idx_list[-1]])\n",
    "    return idx_subspans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eae91162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_idx_spans(span_list):\n",
    "    all_idx = []\n",
    "    for span in span_list:\n",
    "        if type(span)!= str and math.isnan(span):\n",
    "            continue\n",
    "        subspans = span.split(\";\")\n",
    "        for subspan in subspans:\n",
    "            min_lim, max_lim = subspan.split(\",\")\n",
    "            all_idx = all_idx + list(range(int(min_lim), int(max_lim)+1))\n",
    "    all_idx = list(set(all_idx))\n",
    "    all_idx.sort()\n",
    "    \n",
    "    idx_subspans = get_consecutive_subspans(all_idx)\n",
    "    idx_subspans_str = span_to_str(idx_subspans)\n",
    "    return all_idx, idx_subspans, idx_subspans_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d5c8c1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_span_to_tkn_span(idx_subspans, curr_spacy_jsons, isSuperPal):\n",
    "    subt_val = 0 \n",
    "    if isSuperPal and (curr_spacy_jsons['0']['text'] == ' '): # some documents started wiht a space which was considered a separate token, while superPAL remove them. So, should be treated accordingly\n",
    "        subt_val = 1\n",
    "    tkn_subspans = []\n",
    "    for idx_subspan in idx_subspans:\n",
    "        tkn_min_lim = [key for key, value in curr_spacy_jsons.items() if value['idx']-subt_val==idx_subspan[0]]\n",
    "        tkn_max_lim = [key for key, value in curr_spacy_jsons.items() if value['idx']-subt_val+len(value['text'])==idx_subspan[1]]\n",
    "\n",
    "\n",
    "        # if len(tkn_max_lim) != 1 or len(tkn_min_lim) != 1:\n",
    "        #     print(\"gotcha\")\n",
    "        tkn_min_lim, tkn_max_lim = tkn_min_lim[0], tkn_max_lim[0]\n",
    "        tkn_subspans.append([tkn_min_lim, tkn_max_lim])\n",
    "    return tkn_subspans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eb74cb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_CONTENT_POS = [\"AUX\", \"DET\", \"ADP\", \"SCONJ\", \"CONJ\", \"CCONJ\", \"PUNCT\", \"SYM\", \"X\", \"SPACE\"]\n",
    "def calc_agreement(mturk_tkn_subspans, superpal_tkn_subspans, curr_spacy_jsons):\n",
    "    mturk_tkns = set([tkn_id for subspan in mturk_tkn_subspans for tkn_id in range(int(subspan[0]), int(subspan[1])+1) if not curr_spacy_jsons[str(tkn_id)]['text'] in string.punctuation and not curr_spacy_jsons[str(tkn_id)]['pos'] in NON_CONTENT_POS])\n",
    "    superpal_tkns = set([tkn_id for subspan in superpal_tkn_subspans for tkn_id in range(int(subspan[0]), int(subspan[1])+1) if not curr_spacy_jsons[str(tkn_id)]['text'] in string.punctuation and not curr_spacy_jsons[str(tkn_id)]['pos'] in NON_CONTENT_POS])\n",
    "    agreement = len(set.intersection(mturk_tkns, superpal_tkns)) / len(set.union(mturk_tkns, superpal_tkns))\n",
    "    return agreement\n",
    "\n",
    "def calc_r_p_f1(mturk_tkn_subspans, superpal_tkn_subspans, curr_spacy_jsons):\n",
    "    mturk_tkns = set([tkn_id for subspan in mturk_tkn_subspans for tkn_id in range(int(subspan[0]), int(subspan[1])+1) if not curr_spacy_jsons[str(tkn_id)]['text'] in string.punctuation and not curr_spacy_jsons[str(tkn_id)]['pos'] in NON_CONTENT_POS])\n",
    "    superpal_tkns = set([tkn_id for subspan in superpal_tkn_subspans for tkn_id in range(int(subspan[0]), int(subspan[1])+1) if not curr_spacy_jsons[str(tkn_id)]['text'] in string.punctuation and not curr_spacy_jsons[str(tkn_id)]['pos'] in NON_CONTENT_POS])\n",
    "    \n",
    "    precision = len(set.intersection(mturk_tkns, superpal_tkns)) / len(superpal_tkns) if len(superpal_tkns)!=0 else 0\n",
    "    recall = len(set.intersection(mturk_tkns, superpal_tkns)) / len(mturk_tkns) if len(mturk_tkns)!=0 else 0\n",
    "    tp = len(set.intersection(mturk_tkns, superpal_tkns))\n",
    "    fp = len(superpal_tkns) - tp\n",
    "    fn = len(mturk_tkns) - tp\n",
    "\n",
    "    if len(superpal_tkns) == 0 and  len(mturk_tkns) == 0 :\n",
    "        precision, recall = 1, 1\n",
    "\n",
    "    f1 = 0 if precision + recall == 0 else 2*precision*recall / (precision + recall)\n",
    "    return {\"precision\":precision, \"recall\":recall, \"F1\":f1, \"true_pos\":tp, \"false_pos\":fp, \"false_neg\":fn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0343c059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_avg(scores):\n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461857ff",
   "metadata": {},
   "source": [
    "## per-sentence agreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4fbd1a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out of index for document LA062290-0172\n",
      "out of index for document LA062290-0172\n",
      "out of index for document LA101990-0114\n",
      "out of index for document LA112790-0154\n",
      "out of index for document SJMN91-06256107\n",
      "out of index for document LA092389-0092\n",
      "out of index for document SJMN91-06236241\n",
      "out of index for document LA101889-0066\n",
      "out of index for document SJMN91-06236241\n",
      "out of index for document LA092189-0225\n",
      "out of index for document LA060590-0086\n",
      "out of index for document SJMN91-06025182\n",
      "out of index for document LA101889-0066\n",
      "out of index for document LA060590-0086\n",
      "out of index for document LA120890-0055\n",
      "out of index for document LA101590-0066\n",
      "out of index for document SJMN91-06025182\n",
      "out of index for document LA112790-0154\n",
      "out of index for document LA101590-0066\n",
      "out of index for document SJMN91-06058250\n",
      "out of index for document AP890119-0221\n",
      "out of index for document AP890121-0050\n",
      "out of index for document LA101990-0114\n",
      "out of index for document AP890121-0050\n",
      "out of index for document LA101889-0108\n",
      "out of index for document SJMN91-06256107\n",
      "out of index for document LA101889-0108\n",
      "out of index for document LA062390-0068\n",
      "out of index for document LA062590-0096\n",
      "out of index for document LA092189-0225\n",
      "out of index for document LA062390-0068\n",
      "out of index for document AP890119-0221\n",
      "out of index for document SJMN91-06312120\n",
      "out of index for document SJMN91-06312120\n",
      "out of index for document SJMN91-06058250\n",
      "out of index for document LA092389-0092\n",
      "out of index for document LA062590-0096\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "summaries =  list(set(mturk_df['summaryFile']))\n",
    "\n",
    "\n",
    "IAA_scores = {}\n",
    "r_p_f1_scores = {}\n",
    "\n",
    "docwise_IAA_scores = {}\n",
    "docwise_r_p_f1_scores = {}\n",
    "\n",
    "for summary in summaries:\n",
    "    curr_mturk_df = mturk_df[mturk_df['summaryFile']==summary]\n",
    "    curr_superpal_df = superpal_df[superpal_df['summaryFile']==summary]\n",
    "    curr_spacy_doc_name = list(curr_superpal_df[\"documentFile\"])[0]\n",
    "    curr_spacy_jsons = spacy_jsons[curr_spacy_doc_name]\n",
    "    SentCharIdx_list = list(set(curr_mturk_df[\"scuSentCharIdx\"]))\n",
    "    SentCharIdx_list.sort()\n",
    "\n",
    "    IAA_scores[f'{summary}'] = {}\n",
    "    r_p_f1_scores[f'{summary}'] = {}\n",
    "\n",
    "    for sent_idx in SentCharIdx_list:\n",
    "        curr_idx_mturk_df = curr_mturk_df[curr_mturk_df[\"scuSentCharIdx\"]==sent_idx]\n",
    "        curr_idx_superpal_df = curr_superpal_df[curr_superpal_df[\"scuSentCharIdx\"]==sent_idx]\n",
    "\n",
    "        mturk_all_idx, mturk_idx_subspans, mturk_idx_subspans_str = get_full_idx_spans(list(curr_idx_mturk_df[\"docSpanOffsets\"]))\n",
    "        superpal_all_idx, superpal_idx_subspans, superpal_idx_subspans_str = get_full_idx_spans(list(curr_idx_superpal_df[\"docSpanOffsets\"]))\n",
    "\n",
    "        try:\n",
    "            mturk_tkn_subspans = idx_span_to_tkn_span(mturk_idx_subspans, curr_spacy_jsons, False)\n",
    "            superpal_tkn_subspans = idx_span_to_tkn_span(superpal_idx_subspans, curr_spacy_jsons, True)\n",
    "        except IndexError:\n",
    "            doc_name = summary.split(\"_\")[0]\n",
    "            print(f\"out of index for document {doc_name}\")\n",
    "            del IAA_scores[f'{summary}']\n",
    "            del r_p_f1_scores[f'{summary}']\n",
    "            break\n",
    "        curr_agreement = calc_agreement(mturk_tkn_subspans, superpal_tkn_subspans, curr_spacy_jsons)\n",
    "        \n",
    "        curr_sent_id = list(curr_idx_mturk_df['scuSentTknId'])[0]\n",
    "        IAA_scores[f'{summary}'][str(curr_sent_id)] = {\"sent_idx\":sent_idx, \"agreement\":curr_agreement}\n",
    "\n",
    "\n",
    "        r_p_f1_scores[f'{summary}'][str(curr_sent_id)] = calc_r_p_f1(mturk_tkn_subspans, superpal_tkn_subspans, curr_spacy_jsons)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    if summary in IAA_scores.keys():\n",
    "        IAA_scores[f'{summary}'][\"average\"] = calc_avg([value[\"agreement\"] for value in IAA_scores[f'{summary}'].values()]) \n",
    "    if summary in r_p_f1_scores.keys():\n",
    "        r_p_f1_scores[f'{summary}'][\"average\"] = {\"precision\":calc_avg([value[\"precision\"] for value in r_p_f1_scores[f'{summary}'].values()]) , \n",
    "                                                  \"recall\":calc_avg([value[\"recall\"] for value in r_p_f1_scores[f'{summary}'].values()]), \n",
    "                                                  \"F1\":calc_avg([value[\"F1\"] for value in r_p_f1_scores[f'{summary}'].values()]),\n",
    "                                                  \"total_tp\":sum([value[\"true_pos\"] for value in r_p_f1_scores[f'{summary}'].values()]),\n",
    "                                                  \"total_fp\":sum([value[\"false_pos\"] for value in r_p_f1_scores[f'{summary}'].values()]),\n",
    "                                                  \"total_fn\":sum([value[\"false_neg\"] for value in r_p_f1_scores[f'{summary}'].values()])}\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e7de64",
   "metadata": {},
   "source": [
    "## per doc agreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7b178f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out of index for document LA062290-0172\n",
      "out of index for document LA062290-0172\n",
      "out of index for document LA101990-0114\n",
      "out of index for document LA112790-0154\n",
      "out of index for document SJMN91-06256107\n",
      "out of index for document LA092389-0092\n",
      "out of index for document SJMN91-06236241\n",
      "out of index for document LA101889-0066\n",
      "out of index for document SJMN91-06236241\n",
      "out of index for document LA092189-0225\n",
      "out of index for document LA060590-0086\n",
      "out of index for document SJMN91-06025182\n",
      "out of index for document LA101889-0066\n",
      "out of index for document LA060590-0086\n",
      "out of index for document LA120890-0055\n",
      "out of index for document LA101590-0066\n",
      "out of index for document SJMN91-06025182\n",
      "out of index for document LA112790-0154\n",
      "out of index for document LA101590-0066\n",
      "out of index for document SJMN91-06058250\n",
      "out of index for document AP890119-0221\n",
      "out of index for document AP890121-0050\n",
      "out of index for document LA101990-0114\n",
      "out of index for document AP890121-0050\n",
      "out of index for document LA101889-0108\n",
      "out of index for document SJMN91-06256107\n",
      "out of index for document LA101889-0108\n",
      "out of index for document LA062390-0068\n",
      "out of index for document LA062590-0096\n",
      "out of index for document LA092189-0225\n",
      "out of index for document LA062390-0068\n",
      "out of index for document AP890119-0221\n",
      "out of index for document SJMN91-06312120\n",
      "out of index for document SJMN91-06312120\n",
      "out of index for document SJMN91-06058250\n",
      "out of index for document LA092389-0092\n",
      "out of index for document LA062590-0096\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "summaries =  list(set(mturk_df['summaryFile']))\n",
    "\n",
    "docwise_IAA_scores = {}\n",
    "docwise_r_p_f1_scores = {}\n",
    "\n",
    "for summary in summaries:\n",
    "    curr_mturk_df = mturk_df[mturk_df['summaryFile']==summary]\n",
    "    curr_superpal_df = superpal_df[superpal_df['summaryFile']==summary]\n",
    "    curr_spacy_doc_name = list(curr_superpal_df[\"documentFile\"])[0]\n",
    "    curr_spacy_jsons = spacy_jsons[curr_spacy_doc_name]\n",
    "    SentCharIdx_list = list(set(curr_mturk_df[\"scuSentCharIdx\"]))\n",
    "    SentCharIdx_list.sort()\n",
    "    # try:\n",
    "    mturk_all_idx, mturk_idx_subspans, mturk_idx_subspans_str = get_full_idx_spans(list(curr_mturk_df[\"docSpanOffsets\"]))\n",
    "    superpal_all_idx, superpal_idx_subspans, superpal_idx_subspans_str = get_full_idx_spans(list(curr_superpal_df[\"docSpanOffsets\"]))\n",
    "    # except:\n",
    "    #     print(\"gotcha\")\n",
    "    try:\n",
    "        mturk_tkn_subspans = idx_span_to_tkn_span(mturk_idx_subspans, curr_spacy_jsons, False)\n",
    "        superpal_tkn_subspans = idx_span_to_tkn_span(superpal_idx_subspans, curr_spacy_jsons, True)\n",
    "    except IndexError:\n",
    "        doc_name = summary.split(\"_\")[0]\n",
    "        print(f\"out of index for document {doc_name}\")\n",
    "        continue\n",
    "    \n",
    "\n",
    "\n",
    "    curr_agreement = calc_agreement(mturk_tkn_subspans, superpal_tkn_subspans, curr_spacy_jsons)\n",
    "    docwise_IAA_scores[f'{summary}'] = {\"agreement\":curr_agreement}\n",
    "    docwise_r_p_f1_scores[f'{summary}'] = calc_r_p_f1(mturk_tkn_subspans, superpal_tkn_subspans, curr_spacy_jsons)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6832f07",
   "metadata": {},
   "source": [
    "## save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "be1d3856",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_content_words = \"_only_content_words\" if NON_CONTENT_POS else \"\"\n",
    "is_filter = \"_filtered\" if indir_superpal.endswith(\"filtered_checkpoint_2000.csv\") else \"\"\n",
    "with open(os.path.join(outdir,f\"IAA_silver_results_docwise{only_content_words}{is_filter}.json\"), \"w\") as outfile:\n",
    "    outfile.write(json.dumps(docwise_r_p_f1_scores))\n",
    "with open(os.path.join(outdir,f\"IAA_silver_results{only_content_words}{is_filter}.json\"), \"w\") as outfile:\n",
    "    outfile.write(json.dumps(r_p_f1_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957d5828",
   "metadata": {},
   "source": [
    "## IAA total average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d5c0ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per sentence agreement avg:0.5067591300317594\n",
      "per doc agreement avg:0.5317787608066044\n"
     ]
    }
   ],
   "source": [
    "per_sent_IAA_avg = sum([elem['average'] for elem in IAA_scores.values()]) / len([elem['average'] for elem in IAA_scores.values()])\n",
    "per_doc_IAA_avg = sum([elem['agreement'] for elem in docwise_IAA_scores.values()]) / len([elem['agreement'] for elem in docwise_IAA_scores.values()])\n",
    "print(f\"per sentence agreement avg:{per_sent_IAA_avg}\\nper doc agreement avg:{per_doc_IAA_avg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f012065",
   "metadata": {},
   "source": [
    "## Precision, Recall, F1 micro and macro average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "05719899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per document:\n",
      "macro_p:0.7686967026110518\n",
      "macro_r:0.640294414640495\n",
      "macro_F1:0.6856260378517571\n",
      "per document:\n",
      "micro_p:0.759537372691179\n",
      "micro_r:0.6306435430700874\n",
      "micro_F1:0.6891151135473768\n"
     ]
    }
   ],
   "source": [
    "# per sent\n",
    "per_sent_macro_p = calc_avg([elem['average']['precision'] for elem in r_p_f1_scores.values()])\n",
    "per_sent_macro_r = calc_avg([elem['average']['recall'] for elem in r_p_f1_scores.values()])\n",
    "per_sent_macro_F1 = calc_avg([elem['average']['F1'] for elem in r_p_f1_scores.values()])\n",
    "\n",
    "\n",
    "per_sent_full_tp = sum([elem['average']['total_tp'] for elem in r_p_f1_scores.values()])\n",
    "per_sent_full_fp = sum([elem['average']['total_fp'] for elem in r_p_f1_scores.values()])\n",
    "per_sent_full_fn = sum([elem['average']['total_fn'] for elem in r_p_f1_scores.values()])\n",
    "\n",
    "per_sent_micro_p = per_sent_full_tp / (per_sent_full_tp + per_sent_full_fp) \n",
    "per_sent_micro_r = per_sent_full_tp / (per_sent_full_tp + per_sent_full_fn) \n",
    "per_sent_micro_F1 = 2*per_sent_micro_p*per_sent_micro_r / (per_sent_micro_p + per_sent_micro_r)\n",
    "\n",
    "# per doc\n",
    "try:\n",
    "    per_doc_macro_p = calc_avg([elem['precision'] for elem in docwise_r_p_f1_scores.values()])\n",
    "except:\n",
    "    print(\"gotcha\")\n",
    "per_doc_macro_r = calc_avg([elem['recall'] for elem in docwise_r_p_f1_scores.values()])\n",
    "per_doc_macro_F1 = calc_avg([elem['F1'] for elem in docwise_r_p_f1_scores.values()])\n",
    "\n",
    "\n",
    "per_doc_full_tp = sum([elem['true_pos'] for elem in docwise_r_p_f1_scores.values()])\n",
    "per_doc_full_fp = sum([elem['false_pos'] for elem in docwise_r_p_f1_scores.values()])\n",
    "per_doc_full_fn = sum([elem['false_neg'] for elem in docwise_r_p_f1_scores.values()])\n",
    "\n",
    "\n",
    "per_doc_micro_p = per_doc_full_tp / (per_doc_full_tp + per_doc_full_fp) \n",
    "per_doc_micro_r = per_doc_full_tp / (per_doc_full_tp + per_doc_full_fn) \n",
    "per_doc_micro_F1 = 2*per_doc_micro_p*per_doc_micro_r / (per_doc_micro_p + per_doc_micro_r)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"per sentence:\\nmacro_p:{per_sent_macro_p}\\nmacro_r:{per_sent_macro_r}\\nmacro_F1:{per_sent_macro_F1}\\n\")\n",
    "\n",
    "# print(f\"per sentence:\\nmicro_p:{per_sent_micro_p}\\nmicro_r:{per_sent_micro_r}\\nmicro_F1:{per_sent_micro_F1}\\n\")\n",
    "\n",
    "print(f\"per document:\\nmacro_p:{per_doc_macro_p}\\nmacro_r:{per_doc_macro_r}\\nmacro_F1:{per_doc_macro_F1}\")\n",
    "\n",
    "print(f\"per document:\\nmicro_p:{per_doc_micro_p}\\nmicro_r:{per_doc_micro_r}\\nmicro_F1:{per_doc_micro_F1}\")\n",
    "\n",
    "\n",
    "# print(f\"per document:\\nmicro_p:{per_doc_macro_p}\\nmicro_r:{per_doc_macro_r}\\nmicro_F1:{per_doc_macro_F1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe239410",
   "metadata": {},
   "source": [
    "# Full data (2001+2002) Agreement Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba5e93e",
   "metadata": {},
   "source": [
    "#### All tokens (excluding punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9ff6b1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "indir = r\"C:\\Users\\aviv\\OneDrive\\Desktop\\controlled_reduction_production\\cleaned_data\"\n",
    "is_filter = \"_filtered\" if indir_superpal.endswith(\"filtered_checkpoint_2000.csv\") else \"\"\n",
    "subdirs = [\"first_batch\", \"second_batch\"]\n",
    "\n",
    "total_data_sentwise = {}\n",
    "total_data_docwise = {}\n",
    "\n",
    "for subdir in subdirs:\n",
    "    with open(os.path.join(indir, subdir, f\"IAA_silver_results{is_filter}.json\"), 'r') as f:\n",
    "         data_sentwise = json.load(f)\n",
    "    with open(os.path.join(indir, subdir, f\"IAA_silver_results_docwise{is_filter}.json\"), 'r') as f:\n",
    "         data_docwise = json.load(f)\n",
    "    if len(set.intersection(set(data_sentwise.keys()), set(total_data_sentwise.keys()))) != 0 :\n",
    "        print(\"same doc in both (sentwise)!\")\n",
    "    if len(set.intersection(set(data_docwise.keys()), set(total_data_docwise.keys()))) != 0 :\n",
    "        print(\"same doc in both (docwise)!\")\n",
    "    total_data_sentwise.update(data_sentwise)\n",
    "    total_data_docwise.update(data_docwise)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d06c0043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agreement over all tokens (excluding punctuation)\n",
      "per document:\n",
      "macro_p:0.7408858889226297\n",
      "macro_r:0.60041122635158\n",
      "macro_F1:0.6470388731056482\n",
      "per document:\n",
      "micro_p:0.735680098811178\n",
      "micro_r:0.5880973538704581\n",
      "micro_F1:0.653662016269531\n"
     ]
    }
   ],
   "source": [
    "# per sent\n",
    "per_sent_macro_p = calc_avg([elem['average']['precision'] for elem in total_data_sentwise.values()])\n",
    "per_sent_macro_r = calc_avg([elem['average']['recall'] for elem in total_data_sentwise.values()])\n",
    "per_sent_macro_F1 = calc_avg([elem['average']['F1'] for elem in total_data_sentwise.values()])\n",
    "\n",
    "\n",
    "per_sent_full_tp = sum([elem['average']['total_tp'] for elem in total_data_sentwise.values()])\n",
    "per_sent_full_fp = sum([elem['average']['total_fp'] for elem in total_data_sentwise.values()])\n",
    "per_sent_full_fn = sum([elem['average']['total_fn'] for elem in total_data_sentwise.values()])\n",
    "\n",
    "per_sent_micro_p = per_sent_full_tp / (per_sent_full_tp + per_sent_full_fp) \n",
    "per_sent_micro_r = per_sent_full_tp / (per_sent_full_tp + per_sent_full_fn) \n",
    "per_sent_micro_F1 = 2*per_sent_micro_p*per_sent_micro_r / (per_sent_micro_p + per_sent_micro_r)\n",
    "\n",
    "# per doc\n",
    "per_doc_macro_p = calc_avg([elem['precision'] for elem in total_data_docwise.values()])\n",
    "per_doc_macro_r = calc_avg([elem['recall'] for elem in total_data_docwise.values()])\n",
    "per_doc_macro_F1 = calc_avg([elem['F1'] for elem in total_data_docwise.values()])\n",
    "\n",
    "\n",
    "per_doc_full_tp = sum([elem['true_pos'] for elem in total_data_docwise.values()])\n",
    "per_doc_full_fp = sum([elem['false_pos'] for elem in total_data_docwise.values()])\n",
    "per_doc_full_fn = sum([elem['false_neg'] for elem in total_data_docwise.values()])\n",
    "\n",
    "\n",
    "per_doc_micro_p = per_doc_full_tp / (per_doc_full_tp + per_doc_full_fp) \n",
    "per_doc_micro_r = per_doc_full_tp / (per_doc_full_tp + per_doc_full_fn) \n",
    "per_doc_micro_F1 = 2*per_doc_micro_p*per_doc_micro_r / (per_doc_micro_p + per_doc_micro_r)\n",
    "\n",
    "\n",
    "print(\"Agreement over all tokens (excluding punctuation)\")\n",
    "\n",
    "# print(f\"per sentence:\\nmacro_p:{per_sent_macro_p}\\nmacro_r:{per_sent_macro_r}\\nmacro_F1:{per_sent_macro_F1}\\n\")\n",
    "\n",
    "# print(f\"per sentence:\\nmicro_p:{per_sent_micro_p}\\nmicro_r:{per_sent_micro_r}\\nmicro_F1:{per_sent_micro_F1}\\n\")\n",
    "\n",
    "print(f\"per document:\\nmacro_p:{per_doc_macro_p}\\nmacro_r:{per_doc_macro_r}\\nmacro_F1:{per_doc_macro_F1}\")\n",
    "\n",
    "print(f\"per document:\\nmicro_p:{per_doc_micro_p}\\nmicro_r:{per_doc_micro_r}\\nmicro_F1:{per_doc_micro_F1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be05f8d",
   "metadata": {},
   "source": [
    "#### Only Content tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "742f7357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "indir = r\"C:\\Users\\aviv\\OneDrive\\Desktop\\controlled_reduction_production\\cleaned_data\"\n",
    "is_filter = \"_filtered\" if indir_superpal.endswith(\"filtered_checkpoint_2000.csv\") else \"\"\n",
    "subdirs = [\"first_batch\", \"second_batch\"]\n",
    "\n",
    "total_data_sentwise = {}\n",
    "total_data_docwise = {}\n",
    "\n",
    "for subdir in subdirs:\n",
    "    with open(os.path.join(indir, subdir, f\"IAA_silver_results_only_content_words{is_filter}.json\"), 'r') as f:\n",
    "         data_sentwise = json.load(f)\n",
    "    with open(os.path.join(indir, subdir, f\"IAA_silver_results_docwise_only_content_words{is_filter}.json\"), 'r') as f:\n",
    "         data_docwise = json.load(f)\n",
    "    if len(set.intersection(set(data_sentwise.keys()), set(total_data_sentwise.keys()))) != 0 :\n",
    "        print(\"same doc in both (sentwise)!\")\n",
    "    if len(set.intersection(set(data_docwise.keys()), set(total_data_docwise.keys()))) != 0 :\n",
    "        print(\"same doc in both (docwise)!\")\n",
    "    total_data_sentwise.update(data_sentwise)\n",
    "    total_data_docwise.update(data_docwise)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e8bc6287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agreement over all tokens (excluding punctuation)\n",
      "per document:\n",
      "macro_p:0.7560208081815802\n",
      "macro_r:0.6053100374624475\n",
      "macro_F1:0.6560313092704906\n",
      "per document:\n",
      "micro_p:0.7524534174916778\n",
      "micro_r:0.5919061384118347\n",
      "micro_F1:0.662593269377189\n"
     ]
    }
   ],
   "source": [
    "# per sent\n",
    "per_sent_macro_p = calc_avg([elem['average']['precision'] for elem in total_data_sentwise.values()])\n",
    "per_sent_macro_r = calc_avg([elem['average']['recall'] for elem in total_data_sentwise.values()])\n",
    "per_sent_macro_F1 = calc_avg([elem['average']['F1'] for elem in total_data_sentwise.values()])\n",
    "\n",
    "\n",
    "per_sent_full_tp = sum([elem['average']['total_tp'] for elem in total_data_sentwise.values()])\n",
    "per_sent_full_fp = sum([elem['average']['total_fp'] for elem in total_data_sentwise.values()])\n",
    "per_sent_full_fn = sum([elem['average']['total_fn'] for elem in total_data_sentwise.values()])\n",
    "\n",
    "per_sent_micro_p = per_sent_full_tp / (per_sent_full_tp + per_sent_full_fp) \n",
    "per_sent_micro_r = per_sent_full_tp / (per_sent_full_tp + per_sent_full_fn) \n",
    "per_sent_micro_F1 = 2*per_sent_micro_p*per_sent_micro_r / (per_sent_micro_p + per_sent_micro_r)\n",
    "\n",
    "# per doc\n",
    "per_doc_macro_p = calc_avg([elem['precision'] for elem in total_data_docwise.values()])\n",
    "per_doc_macro_r = calc_avg([elem['recall'] for elem in total_data_docwise.values()])\n",
    "per_doc_macro_F1 = calc_avg([elem['F1'] for elem in total_data_docwise.values()])\n",
    "\n",
    "\n",
    "per_doc_full_tp = sum([elem['true_pos'] for elem in total_data_docwise.values()])\n",
    "per_doc_full_fp = sum([elem['false_pos'] for elem in total_data_docwise.values()])\n",
    "per_doc_full_fn = sum([elem['false_neg'] for elem in total_data_docwise.values()])\n",
    "\n",
    "\n",
    "per_doc_micro_p = per_doc_full_tp / (per_doc_full_tp + per_doc_full_fp) \n",
    "per_doc_micro_r = per_doc_full_tp / (per_doc_full_tp + per_doc_full_fn) \n",
    "per_doc_micro_F1 = 2*per_doc_micro_p*per_doc_micro_r / (per_doc_micro_p + per_doc_micro_r)\n",
    "\n",
    "\n",
    "print(\"Agreement over all tokens (excluding punctuation)\")\n",
    "\n",
    "# print(f\"per sentence:\\nmacro_p:{per_sent_macro_p}\\nmacro_r:{per_sent_macro_r}\\nmacro_F1:{per_sent_macro_F1}\\n\")\n",
    "\n",
    "# print(f\"per sentence:\\nmicro_p:{per_sent_micro_p}\\nmicro_r:{per_sent_micro_r}\\nmicro_F1:{per_sent_micro_F1}\\n\")\n",
    "\n",
    "print(f\"per document:\\nmacro_p:{per_doc_macro_p}\\nmacro_r:{per_doc_macro_r}\\nmacro_F1:{per_doc_macro_F1}\")\n",
    "\n",
    "print(f\"per document:\\nmicro_p:{per_doc_micro_p}\\nmicro_r:{per_doc_micro_r}\\nmicro_F1:{per_doc_micro_F1}\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f94d22aade4ad6bf95d7f462d54eb6a2a09ce9cefb6181b1c96776a3dd2d871"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('qasrl_parse')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
